{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture outline\n",
    "- Opinion dynamics with stubborn agents\n",
    "- Resistor networks\n",
    "- Discrete-time Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion dynamics with stubborn agents\n",
    "We study how to simulate the linear averaging dynamics on graphs in presence of stubborn agents.\n",
    "\n",
    "We focus on the optimal placement problem, which consists of optimally chosing the position of a stubborn node on the graph in order to maximize its influence on the asymptotic outcome of the dynamics.\n",
    "\n",
    "Let us first summarize the theory in presence of stubborn agents.\n",
    "\n",
    "We are given a network, where the agents $V$ are divided in regular agents $R$ and stubborn agents $S$. The regular agents update their opinion $x(t)$ according to the standard French-DeGroot model, while the stubborn agents do not update their opinion, i.e., $u(t) \\equiv u$.\n",
    "\n",
    "Let $Q=P|_{R \\times R}$ and $E=P|_{R \\times S}$.\n",
    "\n",
    "Thus, the dynamics for the regular agents read:\n",
    "\n",
    "$$\n",
    "x(t+1) = Qx(t) + Eu.\n",
    "$$\n",
    "\n",
    "Under some assumptions (see the lecture notes for details) the dynamics converges to \n",
    "\n",
    "$$\n",
    "x^* = (\\mathbf{I}-Q)^{-1}Eu.\n",
    "$$\n",
    "\n",
    "Note that:\n",
    "- the asymptotic state is not a consensus;\n",
    "- the asymptotic state does not depend on the initial opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We start by implementing the averaging dynamics with stubborn nodes. \n",
    "\n",
    "To illustrate this procedure, we will analyse the following example that involves a $3 \\times 4$ grid graph $\\mathcal G$.\n",
    "\n",
    "First, we construct the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.generators.lattice.grid_graph(dim=[3,4])\n",
    "n_nodes = len(G)\n",
    "print(\"Number of nodes:\", n_nodes)\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "nx.draw_spectral(G, with_labels=True)\n",
    "\n",
    "print(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dictionary that maps the label of nodes  \n",
    "# (from (0,0) to (3,2)) to their index (from 0 to n_nodes-1)\n",
    "indices = dict()\n",
    "for i in range(n_nodes):\n",
    "    indices[list(G.nodes)[i]] = i\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50;\n",
    "    \n",
    "# Stubborn and regular nodes\n",
    "stubborn = [(0,0), (3,2)];\n",
    "stubborn_id = [indices.get(key) for key in stubborn]\n",
    "regular = [node for node in G.nodes if node not in stubborn]\n",
    "regular_id = [id for id in range(n_nodes) if id not in stubborn_id]\n",
    "print(\"Stubborn nodes:\", stubborn, \"\\n\")\n",
    "print(\"Regular nodes:\", regular, \"\\n\")\n",
    "\n",
    "# Input to stubborn nodes\n",
    "u = [0,1]\n",
    "\n",
    "# P matrix\n",
    "A = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "A = A.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(A,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ A\n",
    "\n",
    "# Submatrices\n",
    "# Using ix_ one can construct index arrays that will \n",
    "# index a cross product. \n",
    "# a[np.ix_([1,3],[2,5])] returns the array [[a[1,2] a[1,5]], [a[3,2] a[3,5]]].\n",
    "Q = P[np.ix_(regular_id, regular_id)]\n",
    "E = P[np.ix_(regular_id, stubborn_id)]\n",
    "\n",
    "# Sample a random initial condition for regular nodes\n",
    "ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "# Set the initial condition for the dynamics\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn_id,0] = u;\n",
    "x[regular_id,0] = ic;\n",
    "print(\"Initial condition:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "    x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "x_final = x[:,n_iter-1]\n",
    "for key in indices.keys():\n",
    "    print(key, x_final[indices[key]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(n_nodes):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = np.average(x_final)\n",
    "print(\"Average asymptotic opinion:\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the dynamics does not converge to consensus. Moreover, in contrast with averaging without input stubborn nodes, we can verify that the asymptotic equilibrium does not depend on the initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample another random initial condition for regular nodes\n",
    "ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn_id,0] = u;\n",
    "x[regular_id,0] = ic;\n",
    "print(\"Initial condition:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "    x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "x_final = x[:,n_iter-1]\n",
    "for key in indices.keys():\n",
    "    print(key, x_final[indices[key]])\n",
    "    \n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(n_nodes):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal placement of stubborn nodes\n",
    "Suppose that node $(0,0)$ is stubborn with opinion $u_{(0,0)}=0$. We want to find the optimal position $(i,j)$ of a stubborn node with opinion $1$ in order to maximize the asymptotic average opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple approach is to consider all possible positions $(i,j)$ and pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50;\n",
    "\n",
    "# We will store final opinion vectors and \n",
    "# average of final opinions in dictionaries\n",
    "# where the key is the position (i,j) of the \n",
    "# 1-stubborn agent\n",
    "final_opinions = dict()\n",
    "average_opinion = dict() \n",
    "\n",
    "\n",
    "for (i,j) in G.nodes:\n",
    "    # Position (0,0) is occupied by the 0-stubborn node\n",
    "    if (i,j)==(0,0):\n",
    "        continue\n",
    "        \n",
    "    # Stubborn and regular nodes\n",
    "    stubborn = [(0,0), (i,j)];\n",
    "    stubborn_id = [indices.get(key) for key in stubborn]\n",
    "    regular = [node for node in G.nodes if node not in stubborn]\n",
    "    regular_id = [id for id in range(n_nodes) if id not in stubborn_id]\n",
    "    print(\"Stubborn nodes:\", stubborn)\n",
    "\n",
    "    # Input to stubborn nodes\n",
    "    u = [0,1]\n",
    "\n",
    "\n",
    "    # P matrix\n",
    "    A = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "    A = A.toarray() # convert A to a numpy array\n",
    "    degrees = np.sum(A,axis=1)\n",
    "    D = np.diag(degrees)\n",
    "    P = np.linalg.inv(D) @ A\n",
    "\n",
    "    # Submatrices\n",
    "    Q = P[np.ix_(regular_id, regular_id)]\n",
    "    E = P[np.ix_(regular_id, stubborn_id)]\n",
    "\n",
    "    # Sample a random initial condition for regular nodes\n",
    "    ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "    # Set the initial condition for the dynamics\n",
    "    x = np.zeros((n_nodes,n_iter))\n",
    "    x[stubborn_id,0] = u;\n",
    "    x[regular_id,0] = ic;\n",
    "\n",
    "    for t in range(1,n_iter):\n",
    "        x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "        x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "    final_opinions[(i,j)] = x[:,n_iter-1]\n",
    "    average_opinion[(i,j)] = np.average(final_opinions[(i,j)])\n",
    "    print(\"Average opinion:\", average_opinion[(i,j)], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the dependence of the average asymptotic opinion on the position of the $1$-stubborn node we can plot the grid graph by setting each node's size and color according to the magnitude of the average asymptotic opinion when the $1$-stubborn is placed in such node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a dummy (0,0) entry to the dictionary\n",
    "# to make its size = n_nodes\n",
    "average_opinion[(0,0)] = 0\n",
    "\n",
    "plt.figure(1, figsize=(7,3))\n",
    "nx.draw(G, \n",
    "        pos = nx.spectral_layout(G),\n",
    "        with_labels=True, \n",
    "        node_size = [np.exp(10*average_opinion[node]) for node in G.nodes],\n",
    "        node_color= [average_opinion[node] for node in G.nodes],\n",
    "        font_size=8,\n",
    "        # node's colors are on the red scale\n",
    "        cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal placements of the 1-stubborn player are the maximizers of the final average opinion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the average opinion values from dict_values to numpy array\n",
    "avg = np.fromiter(average_opinion.values(),dtype=float)\n",
    "\n",
    "optimal_place = [place for place in average_opinion.keys() if average_opinion[place]==np.max(avg)]\n",
    "print(\"Optimal placements:\", optimal_place)\n",
    "\n",
    "# print the final opinions under optimal placement\n",
    "opt_final = final_opinions.get(*optimal_place)\n",
    "print(opt_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the asymptotic opinions of the nodes when the stubborn is placed in (1,1)\n",
    "\n",
    "plt.figure(1, figsize=(7,3))\n",
    "nx.draw(G, \n",
    "        pos = nx.spectral_layout(G),\n",
    "        with_labels=True, \n",
    "        node_size = [np.exp(8*opt_final[indices.get(node)]) for node in G.nodes],\n",
    "        node_color= [opt_final[indices.get(node)] for node in G.nodes],\n",
    "        font_size=8,\n",
    "        # node's colors are on the red scale\n",
    "        cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more general model: overview on Friedkin-Johnsen model\n",
    "The French-DeGroot dynamics can be generalized by taking into account that agents are not completely regular or completely stubborn. The opinion of each agent is in part due to \"innate\" opinions, and in part due to influence of society.\n",
    "\n",
    "The following opinion dynamics model is known as Friedkin-Johnsen model.\n",
    "Here:\n",
    "- $x_i(t)$ is the opinion of the agent $i$;\n",
    "- $y_i$ is the innate opinion of agent $i$.\n",
    "- $\\alpha_i$ is its level of stubborness, i.e., the level of confidence in his/her opinion $y_i$.\n",
    "\n",
    "The dynamics reads:\n",
    "\n",
    "$$\n",
    "x_i(t+1) = \\alpha_i y_i + (1-\\alpha_i) \\sum_{j} P_{ij} x_j(t).\n",
    "$$\n",
    "\n",
    "If $\\alpha = \\mathbf{0}$, we get the French-DeGroot model without input.\n",
    "\n",
    "If $\\alpha \\in \\{0,1\\}^{V}$, we get the French-DeGroot model with stubborn nodes.\n",
    "\n",
    "As the French-DeGroot model with input, also the Friedkin-Johnsen dynamics converges to a non-consensus state, which depends on $\\alpha, y$, but not on the initial opinions $x(0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resistor networs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given an undirected weighted graph, where the weight $c_{ij}$ denotes the conductance between nodes $i$ and $j$. \n",
    "Let $x$ indicate the voltage distribution, and $\\phi$ denote the current flowing in the network. Ohm's law establish the following relation between current, voltage and conductances: \n",
    "\n",
    "$$\n",
    "\\phi_{ab} = c_{ab}(x_a-x_b).\n",
    "$$ \n",
    "\n",
    "We here recall the main instruments to solve electrical network problems: **Series law, Parallel law, Gluing**.\n",
    "\n",
    "![figure](reti_elettriche.png)\n",
    "\n",
    "**Observation**: asymptotic opinions in French-DeGroot model with stubborn agents is equivalent to voltage in resistor network where the voltage of some nodes is fixed a priori (boundary conditions).\n",
    "\n",
    "Indeed, they satisfy same equations, i.e., for every regular node (non-stubborn in opinion dynamics, without boundary conditions in electrical problems), \n",
    "\n",
    "$$x_i = \\sum_j P_{ij} x_j$$ \n",
    "\n",
    "We can thus use this equivalence to solve a problem with the most convenient tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: two equivalent problems**\n",
    "\n",
    "**Problem 1**\n",
    "We are given a network $G=(V,E,W)$, where $W$ is the conductance matrix of the network (we consider in this case unweighted graphs). \n",
    "Let $x_0=0, x_4=1$ be the boundary conditions on the voltage. Find the voltage of the other nodes.\n",
    "\n",
    "**Problem 2**\n",
    "We are given a social network $G=(V,E,W)$. Let $\\{0,4\\}$ the set of stubborn agents, with opinions $u_0 = 0, u_4 = 1$. Find the asymptotic opinions of French-DeGroot dynamics.\n",
    "\n",
    "The solution of the problem is the same. Thus, we can solve only one of the two by using tools from opinion dynamics or electrical network based on what fits better with the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from([(0,1),(1,2),(2,3),(3,0),(0,4),(1,4),(2,4),(3,4)])\n",
    "\n",
    "pos = {0: (0,0), 1: (2,0), 2: (2,2), 3: (0,2), 4: (1,1)}\n",
    "\n",
    "nx.draw(G,pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 methods for solving the problem**\n",
    "\n",
    "**Method 1**\n",
    "Solve the system of linear equations $x_i=\\sum_j P_{ij} x_j$ for every regular node $i$..\n",
    "\n",
    "**Method 2**\n",
    "Run the French-DeGroot dynamics until convergence.\n",
    "\n",
    "**Method 3**\n",
    "Use Series, parallel, and gluing laws.\n",
    "\n",
    "We skip Method 1, let us start with **Method 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray()\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "n_nodes = G.number_of_nodes()\n",
    "\n",
    "stubborn = [0,4];\n",
    "regular = [node for node in G.nodes if node not in stubborn]\n",
    "\n",
    "print(\"Stubborn nodes:\", stubborn, \"\\n\")\n",
    "print(\"Regular nodes:\", regular, \"\\n\")\n",
    "\n",
    "# Input to stubborn nodes\n",
    "u = [0,1]\n",
    "\n",
    "# Submatrices\n",
    "Q = P[np.ix_(regular, regular)]\n",
    "E = P[np.ix_(regular, stubborn)]\n",
    "\n",
    "# Set the initial condition for the dynamics\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn,0] = u;\n",
    "print(\"Initial condition:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular, t] = Q @ x[regular, t-1] + E @ x[stubborn, t-1]\n",
    "    x[stubborn, t] = x[stubborn, t-1];\n",
    "\n",
    "x_final = x[:,n_iter-1]\n",
    "print(\"final opinions:\", x_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 3**\n",
    "\n",
    "Note by symmetry that $x_1 = x_3$, then we can operate **gluing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = nx.Graph()\n",
    "G1.add_edges_from([(0,1),(1,2),(0,4),(1,4),(2,4)])\n",
    "\n",
    "labels = ['2','1','2','2','1']\n",
    "\n",
    "zip_operator = zip(G1.edges(), labels)\n",
    "labels = dict(zip_operator)\n",
    "\n",
    "plt.subplot(121)\n",
    "nx.draw(G,pos, with_labels=True)\n",
    "plt.subplot(122)\n",
    "nx.draw_networkx_edge_labels(G1,pos,edge_labels = labels)\n",
    "nx.draw(G1,pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new node 1 is the result of gluing 1 and 3.\n",
    "\n",
    "We thus have conductances:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "c_{04} = c_{24} = 1, \\\\\n",
    "c_{01} = c_{12} = c_{14} = 2.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We operate a series composition of edges $(2,4)$ and $(1,2)$, plus a parallel composition of the resulting link with $(1,4)$. The resulting conductance $c_{14}$ is\n",
    "\n",
    "$$\n",
    "c'_{14} = (c^{-1}_{24}+c_{21}^{-1})^{-1} + c_{14} = (2^{-1}+1^{-1})^{-1} + 2 = 2/3 + 2 = 8/3\n",
    "$$\n",
    "\n",
    "and the resulting network is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2 = nx.Graph()\n",
    "G2.add_edges_from([(0,1),(0,4),(1,4)])\n",
    "\n",
    "# plot old graph\n",
    "plt.subplot(121)\n",
    "nx.draw_networkx_edge_labels(G1,pos,edge_labels = labels)\n",
    "nx.draw(G1,pos, with_labels=True)\n",
    "\n",
    "# plot new graph\n",
    "plt.subplot(122)\n",
    "\n",
    "labels = ['2','1','8/3']\n",
    "zip_operator = zip(G2.edges(), labels)\n",
    "labels = dict(zip_operator)\n",
    "nx.draw_networkx_edge_labels(G2,pos,edge_labels = labels)\n",
    "nx.draw(G2,pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have conductances:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "c_{04} = 1, \\\\\n",
    "c_{14} = (1/2 + 1)^{-1} + 2 = 8/3, \\\\\n",
    "c_{01} = 2.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The resulting conductance of the series of links $(0,1)$ and $(1,4)$ is $(1/2+3/8)^{-1} = 8/7$.\n",
    "\n",
    "Using the fact that $x_0 = 0, x_4 = 1$, by Ohm's law, $\\phi_{41}=\\phi_{10}=8/7$.\n",
    "\n",
    "Using again Ohm's law:\n",
    "\n",
    "$$\n",
    "2(x_1-x_0) = \\phi_{10} \\implies x_1 = 8/14 = 4/7 \n",
    "$$\n",
    "\n",
    "Moreover, by symmetry, $x_3 = x_1 = 4/7$.\n",
    "\n",
    "Finally, by Kirchhoff's law on $G1$ (current entering node 2 is equal to the current leaving node 2),\n",
    "\n",
    "$$\n",
    "(x_4 - x_2)C_{24} = (x_2-x_1)C_{12} \\implies x_2 = \\frac{x_4 C_{24} + x_1C_{12}}{C_{24}+C_{12}} = \\frac{1+\\frac{4}{7}\\cdot2}{1+2} = \\frac{5}{7},\n",
    "$$\n",
    "\n",
    "which is equivalent to what obtained by French-DeGroot dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voltage\n",
    "x = np.array([0,4/7,5/7,4/7,1])\n",
    "\n",
    "print('voltage:', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voltage is equivalent to asymptotic opinions in French-DeGroot dynamics with stubborn nodes.\n",
    "\n",
    "We can thus apply techniques from De Groot dynamics (e.g., iterative implementation of the dynamics) to find the voltage in electrical network, or apply techinques from electrical networks (e.g., series and parallel composition, gluing, Ohm's law) to find asymptotic opinions in De Groot dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example: Random Walks on graphs and flow dynamics\n",
    "In this section we study a first example of discrete time Markov chain, which is the simple random walk on a graph, and we analyse the connection between random walks and flow dynamics.\n",
    "\n",
    "To explore such connections we first learn how to simulate a random walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Random Walk\n",
    "\n",
    "A random walker on a graph $\\mathcal G$ is an agent that starts at the initial time $0$ at some node and at each time moves from the current position to a neighboring one, chosen with uniform probability (this can be generalized).\n",
    "\n",
    "To learn how to simulate a random walk, here we consider the example of a $n \\times n$ chessboard with a single knight on it. \n",
    "1. We construct a network $G$ with all knight's possible moves. In this network nodes represent chessboard locations and an edge between two locations is present if the knight is admitted to move from one to another. (Note that the resulting graph is undirected).\n",
    "2. We implement a simulation of the knight's random walk on the chessboard network $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice, rand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Knight's Network\n",
    "\n",
    "Here we define function `GenerateKnightNetwork` that constructs the knight's network. \n",
    "It exploits two auxiliary functions, `ApplyLegalMoves` and `isLegalPos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines if the position obtained applying a move is legal,\n",
    "# i.e. if it is inside the chessboard\n",
    "def isLegalPos(x,boardSize):\n",
    "    if x >= 0 and x < boardSize:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Apply the knight's legal moves to the current position to construct\n",
    "# neighboring nodes in the knight's graph\n",
    "\n",
    "def ApplyLegalMoves(x,y,boardSize):\n",
    "    # will store the neighboring nodes\n",
    "    new_positions = []\n",
    "    \n",
    "    # offsets describe the effect of the knight's legal moves\n",
    "    # on the current position's row and column\n",
    "    offsets = [(-1,-2),(-1,2),(-2,-1),(-2,1),\n",
    "                   ( 1,-2),( 1,2),( 2,-1),( 2,1)]\n",
    "    \n",
    "    # for each legal move, compute the new position's row and column\n",
    "    for off in offsets:\n",
    "        new_x = x + off[0]\n",
    "        new_y = y + off[1]\n",
    "        \n",
    "        # if the new position doesn't exceed the boardsize,\n",
    "        # accept it as legal\n",
    "        if isLegalPos(new_x,boardSize) and isLegalPos(new_y,boardSize):\n",
    "            new_positions.append((new_x,new_y))\n",
    "         \n",
    "    return new_positions\n",
    "\n",
    "# Generates the graph representing the knigth network.\n",
    "# Return both the graph object G and the pos dictionary\n",
    "# for drawing G.\n",
    "\n",
    "def GenerateKnightNetwork(boardSize):\n",
    "    # undirected graph G will store the knight's network\n",
    "    G = nx.Graph()\n",
    "    # when drawing G, the pos dictionary describes the position on \n",
    "    # a boardsize x boardsize grid where to place nodes\n",
    "    pos = {}\n",
    "    \n",
    "    # we assign position to nodes\n",
    "    # we have boardSize x boardSize nodes\n",
    "    for row in range(boardSize):\n",
    "        for column in range(boardSize):\n",
    "            node_id = row + column*boardSize\n",
    "            # pos[node_id] are the (x,y) coordinates of node node_id \n",
    "            # on a square grid of side boardSize\n",
    "            pos[node_id] = np.array([1.0*row/boardSize, 1.0*column/boardSize])\n",
    "            \n",
    "            # compute the (row,column) of neighboring position to the\n",
    "            # current one, i.e., positions on the chessoboard reachable\n",
    "            # by applying legal moves\n",
    "            neigh_pos = ApplyLegalMoves(row, column, boardSize)\n",
    "            # for each neigbhoring position, compute the id and add\n",
    "            # a link in G from current position node_id to neigh_id\n",
    "            for p in neigh_pos:\n",
    "                neigh_id = p[0] + p[1]*boardSize\n",
    "                G.add_edge(node_id, neigh_id)\n",
    "    return G, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to generate an example of the knight's network, with a specified boardsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardSize = 8\n",
    "# use position dictionary returned by the function GenerateKnightNetwork\n",
    "(G,pos) = GenerateKnightNetwork(boardSize)\n",
    "nx.draw(G,pos, with_labels=True, node_color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate the Random Walk Process\n",
    "Now that we have the graph $G$ representing the knight's network, we can simulate the knight's random walk on it. \n",
    "\n",
    "The walk starts at some given node and it can either terminate after a specified number of steps or when it first returns to the starting node.\n",
    "\n",
    "At each step, the walker is at some node `xi` and has to decide which node to visit next. \n",
    "\n",
    "In this simple version of the random walk, he does it by choosing a neighbor of the current node uniformly at random.\n",
    "More formally, he looks at row `xi` of the normalized weight matrix $P$ of the graph $G$ and interprets the numbers on that row as the probability of visiting the corresponding nodes next, given that he currently is at `xi`.\n",
    "\n",
    "To simulate the random walk we define the function `RandomWalk`, which allows to specify the graph $G$ on which the walk takes place, the starting node and the stopping criterion.\n",
    "\n",
    "**Remark 1**: the `RandomWalk` function is independent on the specific example we are studying. In other words, it allows to simulate any simple random walk on any **unweighted** graph $G$, since it only exploits the general features of the stochastic process at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulates a random walk on the graph G, starting from node xi.\n",
    "# if till_first_return = True the random walk stops the first time\n",
    "# it returns to the starting node xi.\n",
    "# Otherwise, it goes on for num_steps steps.\n",
    "\n",
    "def RandomWalk(G, xi, num_steps, till_first_return = False):\n",
    "    # nodeSeq stores the sequence of visited nodes\n",
    "    nodeSeq = []\n",
    "    nodeSeq.append(xi)\n",
    "    \n",
    "    # if the walk ends at the first return to xi\n",
    "    if till_first_return:\n",
    "        # stores the initial position to check if the \n",
    "        # walk returns to it\n",
    "        x_init = xi\n",
    "        \n",
    "        # no upper bound on the number of steps\n",
    "        while True:\n",
    "            # compute the next visited node xi by chosing uniformly\n",
    "            # at random a neighbor of the current one\n",
    "            xi = choice(G.adj[xi],1)[0]     \n",
    "            nodeSeq.append(xi)\n",
    "            \n",
    "            # check if the walk has returned to the starting node\n",
    "            # if so, end the walk\n",
    "            if xi == x_init:\n",
    "                return nodeSeq\n",
    "    \n",
    "    # if the walk ends after num_steps steps\n",
    "    else:\n",
    "        for i in range(num_steps):\n",
    "            xi = choice(G.adj[xi],1)[0]      \n",
    "            nodeSeq.append(xi)\n",
    "        return nodeSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement RandomWalk with different stopping criteria, or using non-uniform transition probability distributions (for weighted graph, as we shall see later on).\n",
    "\n",
    "As a first experiment, we simulate a simple random walk on $G$ with $10$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a random walk on G \n",
    "nodeSeq = RandomWalk(G, xi=0, num_steps=10, till_first_return=False)\n",
    "edgeSeq = [(nodeSeq[i-1], nodeSeq[i]) for i in range(1,len(nodeSeq))]\n",
    "\n",
    "# Draw G and represent the random walk by colouring the edge sequence\n",
    "# first draw all nodes and links\n",
    "nx.draw(G, pos)\n",
    "# then, on the previous picture, add node labels and highlight the edge sequence\n",
    "nx.draw(G, pos, with_labels=True, edgelist = edgeSeq, edge_color='blue', node_color='red', width=2)\n",
    "\n",
    "print(\"Node sequence:\", nodeSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a random walk on G that stops at the first return time\n",
    "# note that if till_first_return = True, 'num_steps' is negligible\n",
    "nodeSeq = RandomWalk(G, xi=0, num_steps=1, till_first_return=True)\n",
    "edgeSeq = [(nodeSeq[i-1], nodeSeq[i]) for i in range(1,len(nodeSeq))]\n",
    "\n",
    "# Draw G and represent the random walk by colouring the edge sequence\n",
    "# first draw all nodes and links\n",
    "nx.draw(G, pos)\n",
    "# then, on the previous picture, add node labels and highlight the edge sequence\n",
    "nx.draw(G, pos, with_labels=True, edgelist = edgeSeq, edge_color='blue', node_color='red', width=2)\n",
    "\n",
    "# if the node sequence is not deducible from the plot, you can print the nodeSeq\n",
    "print(\"Node sequence:\", nodeSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a random walk on G with 200 steps\n",
    "nodeSeq = RandomWalk(G, xi=0, num_steps=200, till_first_return=False)\n",
    "edgeSeq = [(nodeSeq[i-1], nodeSeq[i]) for i in range(1,len(nodeSeq))]\n",
    "\n",
    "# Draw G and represent the random walk by colouring the edge sequence\n",
    "# first draw all nodes and links\n",
    "nx.draw(G, pos)\n",
    "# then, on the previous picture, add node labels and highlight the edge sequence\n",
    "nx.draw(G, pos, with_labels=True, edgelist = edgeSeq, edge_color='blue', node_color='red', width=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even with a larger number of steps some of the nodes may not be visited!\n",
    "\n",
    "**Question**: do you think that all the nodes have the same probability of being visited?\n",
    "\n",
    "To answer this question, we need to relate random walks to flow dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walks and the Flow Dynamics\n",
    "\n",
    "The random walk and the flow dynamics are deeply connected. Indeed, if we describe the position of the walker on $G$ at time $t$ with a random variable $x(t)$, the variable's probability distribution evolves according to a flow dynamics. Let\n",
    "\n",
    "$$\n",
    "\\pi_i(t) = \\mathbf{P}\\{x(t)=i\\}.\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\pi(t+1) = P'\\pi(t)\n",
    "$$\n",
    "\n",
    "or more explicitly\n",
    "\n",
    "$$\n",
    "\\pi_j(t+1) = \\sum_{i \\in \\mathcal V} P_{ij}\\pi_i(t),\n",
    "$$\n",
    "\n",
    "Moreover, one can use a random walk to estimate the invariant measure $\\pi$ of the graph $G$ (assume here $G$ is strongly connected). Indeed, by the Katz theorem, the fraction of time spent by the walker on each node tends to the node's value in the invariant measure $\\pi$ as the length of the walk increases.\n",
    "\n",
    "The following section shows how to compute empirical frequencies and how to compare them with the inviariant distribution of $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical frequencies and invariant distribution\n",
    "The empirical frequencies are the fractions of total walk time that each node of G is visited in the random walk. \n",
    "\n",
    "They can be represented by a histogram as follows.\n",
    "\n",
    "We simulate random walks starting at each node of $G$, and we keep track of the sequence of visited nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nodeSeq: list to store all random walks\n",
    "nodeSeq = []\n",
    "\n",
    "# simulate one random walk for each initial node in G\n",
    "for xi in range(G.number_of_nodes()):\n",
    "    # list.extend extends the list by appending all the items from an iterable\n",
    "    nodeSeq.extend(RandomWalk(G, xi, 100))\n",
    "    \n",
    "# plt.hist computes and draws the histogram of nodeSeq. We have one bin for each node,\n",
    "# so each bin width is 1 and the number of observations in each bin equals the number\n",
    "# of visits to the correspnding node. \n",
    "# Since density=True, the return element will be the counts normalized \n",
    "# to form a probability density.\n",
    "h = plt.hist(nodeSeq, bins = G.number_of_nodes(), density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use empirical frequencies to approximate the invariant measure of $G$, we have to construct long random walks. As an example, here we run a random walk with $10000$ steps for each initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodeSeq: list to store all random walks\n",
    "nodeSeq = []\n",
    "\n",
    "# simulate one random walk for each initial node in G\n",
    "for xi in range(G.number_of_nodes()):\n",
    "    # list.extend extends the list by appending all the items from an iterable\n",
    "    nodeSeq.extend(RandomWalk(G, xi, 10000))\n",
    "    \n",
    "# plt.hist computes and draws the histogram of nodeSeq. We have one bin for each node,\n",
    "# so each bin width is 1 and the number of observations in each bin equals the number\n",
    "# of visits to the correspnding node. \n",
    "# Since density=True, the return element will be the counts normalized \n",
    "# to form a probability density.\n",
    "h = plt.hist(nodeSeq, bins = G.number_of_nodes(), density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequencies are now more smooth, and look not uniform, as expected. Let us compare frequencies with the invariant distribution of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a \"long\" random walk\n",
    "\n",
    "nodeSeq = RandomWalk(G, 0, 100000, False)\n",
    "\n",
    "# Compute empirical frequencies\n",
    "\n",
    "frequencies = np.zeros(len(G))\n",
    "# count the visits to each node\n",
    "for node in nodeSeq:\n",
    "    frequencies[node] += 1\n",
    "# normalize the counts to obtain frequencies\n",
    "frequencies /= len(nodeSeq)\n",
    "print(\"Frequencies:\", frequencies, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the nodes to compare empirical frequencies with invariant distribution\n",
    "H = nx.Graph()\n",
    "H.add_nodes_from(sorted(G.nodes(data=True)))\n",
    "H.add_edges_from(G.edges(data=True))\n",
    "\n",
    "print(H.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute P matrix\n",
    "A = nx.adjacency_matrix(H) # -> return type is scipy.sparse.csr_matrix\n",
    "A = A.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(A,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ A\n",
    "\n",
    "# Compute invariant distribution\n",
    "values,vectors = np.linalg.eig(P.T)\n",
    "index = np.argmax(values.real)\n",
    "pi = vectors[:,index].real\n",
    "pi = pi/np.sum(pi)\n",
    "\n",
    "print(\"pi=\", pi, \"\\n\")\n",
    "\n",
    "print(\"frequencies=\", frequencies, \"\\n\")\n",
    "\n",
    "# Evaluate the approximation error by computing the norm of\n",
    "# the difference between the empirical frequencies and the \n",
    "# invariant measure\n",
    "error = np.linalg.norm(frequencies-pi)\n",
    "print(\"Error\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** Note how important the invariant distribution is. So far, it appeared in many different contexts:\n",
    "- centrality measures in social networks;\n",
    "- consensus value in averaging dynamics;\n",
    "- asymptotic limit of linear flow dynamics;\n",
    "- invariant probability distributions in random walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Consider the graph shown in the following picture:\n",
    "![Exgraph](graph1.png)\n",
    "\n",
    "**Remark 2**: for nodes without out-going links, follow the convention and add a self-loop.\n",
    "\n",
    "1. For each node of $G$, simulate the simple random walk starting from that node. What do you observe? How can you justify your observations?\n",
    "2. Study the graph $G$ and compute its invariant measures. Can you see any relation with the behavior of the random walk?\n",
    "3. Substitute the link (5,1) with the link (1,5) and repeat the analysis performed at point 1. and 2. What topological property of the graph has changed? How does this reflect on the random walk process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete time Markov chains\n",
    "We already saw an example of discrete time Markov chain: the random walk on the knight's network.\n",
    "\n",
    "In general, every discrete time Markov chain can be interpreted as a random walk on a weighted directed graph. So, in the general case, transition probabilities from each node to its neighbors are not uniformly distributed.\n",
    "\n",
    "To better understand this notion, we analyse the following example.\n",
    "\n",
    "![DMCgraph](discreteMC.png)\n",
    "\n",
    "1. Construct the directed graph G with weights as shown in the picture.\n",
    "2. Compute the invariant probability distribution vector by computing the leading eigenvector of $P'$.\n",
    "3. Simulate a random walk starting from node 1 on the graph for n = 1000, 2000, 5000, 10000 steps. Determine the fraction of the steps the walk has been in each node i. Compare this with the invariant distribution. What do you observe?\n",
    "\n",
    "**Hint:** you can adopt the function RandomWalk by modifying the choice of the next node to visit according to the fact that the transition probability is not uniform.\n",
    "\n",
    "4. What happens with your estimate of $\\pi$ if you remove node 5 and all links connected to it from the graph and add a self loop of weight 1 to node 6?\n",
    "5. Compute the expected hitting time $\\mathbb{E}_j[T_S]$, $\\forall j \\in R = \\mathcal V \\setminus \\mathcal S$, for the set $S = \\{2, 5\\}$ analytically.\n",
    "6. For every node i, simulate several times a random walk on G that begins in node i and stops when it comes back to it. Use this simulation to estimate the expected return time, $\\mathbb{E}_i[T_i^+]$. Compare this estimate with the expected return times obtained analytically from the expected hitting times.\n",
    "\n",
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "# add weighted edges\n",
    "G.add_weighted_edges_from([(1,3,1),(1,4,2),(2,1,1),(3,1,7),(3,6,3),(4,2,1),(4,3,4),(5,5,2),(5,3,1),(6,5,1)])\n",
    "\n",
    "# define a new graph with sorted nodes\n",
    "H = nx.DiGraph()\n",
    "H.add_nodes_from(sorted(G.nodes(data=True)))\n",
    "H.add_edges_from(G.edges(data=True))\n",
    "\n",
    "nx.draw(H,with_labels=True)\n",
    "\n",
    "# Compute P matrix\n",
    "A = nx.adjacency_matrix(H) # -> return type is scipy.sparse.csr_matrix\n",
    "A = A.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(A,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ A\n",
    "\n",
    "n_nodes = H.number_of_nodes()\n",
    "\n",
    "print(\"P:\", P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. If node 5 is removed, node 6 is a sink node. Hence, all random walks will eventually reach this node so that the estimate of the invariant measure will converge to $\\pi =[0, 0, 0, 0, 1]$. You can check this by simulating the random walk on the modified graph.\n",
    "\n",
    "The interpretation for this is that as the random walks hits 6, it cannot escape. Thus, from there on, it will stay in 6 forever. In the limit of infinite $t$, the fraction of time spent in node 6 tends to 1. This is coeherent with the fact that invariant distribution centrality in a graph has support only on nodes that belong to trapping set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The expected hitting times  $\\hat{x}= (\\mathbb{E}_i[T_S])_{i \\in R}$ for the set $S$ and for all nodes $i \\in R = \\mathcal V \\setminus S$ can be computed by solving the system of equations\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\mathbf{1} + \\hat{P}\\hat{x},\n",
    "$$ \n",
    "\n",
    "where $\\hat{P}$ is obtained from $P$ (the normalized weight matrix of the graph) by removing the rows and columns corresponding to the nodes in the set $S$.\n",
    "\n",
    "More explicitly, the expected hitting times can be expressed as\n",
    "\n",
    "$$\n",
    "\\hat{x} = (I - \\hat{P})^{-1} \\mathbf{1}\n",
    "$$\n",
    "\n",
    "**Remark**: note that $(I - \\hat{P})$ is invertible only if $V \\setminus S$ has at least a link pointing to $S$. Indeed, if $(I - \\hat{P})$ is not invertible. the random walk starting from nodes in $V \\setminus S$ cannot hit nodes in $S$, and the hitting times diverge.\n",
    "\n",
    "Thus, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set S and the remaining nodes R\n",
    "# Subtract -1 because indexes go from 0 to 5 and nodes from 1 to 6\n",
    "S = [1, 4] # refer to nodes [2,5]\n",
    "R = [node for node in range(n_nodes) if node not in S]\n",
    "\n",
    "# Restrict P to R x R to obtain hat(P)\n",
    "hatP = P[np.ix_(R, R)]\n",
    "\n",
    "# solve the linear system to obtain hat(x)\n",
    "# np.linalg.solve solves a linear matrix equation given\n",
    "# the coefficient matrix and the dependent variable values\n",
    "hatx = np.linalg.solve((np.identity(n_nodes-2)-hatP),np.ones(n_nodes-2))\n",
    "# map node to position of node in hatx\n",
    "map = {0: 0, 2: 1, 3: 2, 5: 3}\n",
    "\n",
    "# define the hitting times to the set S\n",
    "# hitting time is 0 if the starting node is in S\n",
    "hitting_s = np.zeros(n_nodes)\n",
    "# hitting time is hat(x) for nodes in R\n",
    "for r in R:\n",
    "    hitting_s[r] = hatx[map[r]]\n",
    "\n",
    "print(\"hitting times:\", hitting_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for node 6 the expected hitting time is 1, because its only outneighbour belongs to $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. To compute expected return times analytically, recall that they can be caracterized by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_i[T_i^+] = 1 + \\sum_{j} P_{ij} \\mathbb{E}_j[T_i]\n",
    "$$\n",
    "\n",
    "where $\\mathbb{E}_j[T_i]$ is the expected hitting time to the set $S={i}$ starting from $j$.\n",
    "\n",
    "So, for computing $\\mathbb{E}_i[T_i^+]$, one can:\n",
    "- set $S=\\{i\\}$\n",
    "- compute the expected hitting times to $S$, $\\mathbb{E}_j[T_i]$, $\\forall j \\in V\\setminus \\{i\\}$ (as done in point 5)\n",
    "- apply the linear relation $\\mathbb{E}_i[T_i^+] = 1 + \\sum_{j} P_{i,j} \\mathbb{E}_j[T_i]$\n",
    "\n",
    "Instead, an estimation of the expected return time $\\mathbb{E}_i[T_i^+]$, $\\forall i$, is obtained by simulating random walks that start at $i$ and end at the first return (you can use `RandomWalk(G, xi=i, num_steps = 'inf', till_first_return = True)`)\n",
    "\n",
    "Implement and compare the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
