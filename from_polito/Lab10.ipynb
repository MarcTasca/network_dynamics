{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy best response and best response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-time best response dynamics\n",
    "Consider a game $\\left(\\mathcal V, \\mathcal A, \\{u_i\\}_{i \\in \\mathcal V}\\right)$. \n",
    "\n",
    "The **continuous-time asynchronous best response dynamics** is a continuous time Markov chain $X(t)$ with state space $\\mathcal X= \\mathcal A^{\\mathcal V}$ coinciding with the configuration space of the game and transition rate matrix $\\Lambda$ as follows: \n",
    "\n",
    "$\\Lambda_{xy} = 0$ for every two configurations $x, y \\in \\mathcal X$ that differ in more\n",
    "than one entry, and\n",
    "\n",
    "$$\n",
    "\\Lambda_{xy} = \\begin{cases}\n",
    "|B_i(x_{-i})|^{-1} \\quad &\\text{if} \\quad y_i \\in B_i(x_{-i}) \\\\\n",
    "0 \\quad &\\text{if} \\quad y_i \\notin B_i(x_{-i})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "for every two configurations $x, y \\in \\mathcal X$ differing in entry $i$ only, i.e., such that\n",
    "$x_i \\neq y_i$ and $x_{-i} = y_{-i}$.\n",
    "\n",
    "## Continuous-time best response dynamics in potential games\n",
    "\n",
    "**Proposition**: consider a potential game, and let $\\mathcal{N}$ be the set of the Nash equilibria. Then, for every distribution of the initial configuration $X(0)$, the best response dynamics $X(t)$ converges to $\\mathcal{N}$ in finite time with probability 1. \n",
    "\n",
    "**Remark**: Convergence is in fact insured to a particular subset of NE, that is the one consisting of the nodes of all the sink connected components of the underlying transition graph of the Markov process. In the sequel we denote such subset as $\\mathcal{N}_\\infty$: it is a trapping set and actually the largest trapping set inside $\\mathcal{N}$. We call $\\mathcal{N}_\\infty$ the set of **recurrent Nash equilibria**. Notice that in those special cases when $\\mathcal{N}_\\infty = \\text{argmax}(\\Phi(x))$, we actually have that the BR dynamics converges to the subset of Nash equilibria consisting of the maxima of the potential.\n",
    "\n",
    "The continuous-time best response dynamics has a similar behaviour to discrete-time asynchronous best-response. We do not implement here the continuous-time best response dynamics. We instead focus on **continuous-time noisy best reponse dynamics**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-time noisy best response dynamics\n",
    "\n",
    "We consider a **continuous-time asynchronous noisy best response  dynamics** with inverse noise parameter $\\eta$, i.e., a continuous time Markov chain on the configuration space $\\mathcal X$ (the set of all game's configurations) with transition rates $\\Lambda_{xy} = 0$ for every two configurations $x, y \\in \\mathcal X$ that differ in more\n",
    "than one entry, and\n",
    "\n",
    "$$\n",
    "\\Lambda_{xy} = \\frac{\\exp^{\\eta u_i(y_i,x_{-i})}}{\\sum_{a \\in \\mathcal A} \\exp^{\\eta u_i(a,x_{-i})} }\n",
    "$$\n",
    "\n",
    "for every two configurations $x, y \\in \\mathcal X$ differing in entry $i$ only, i.e., such that\n",
    "$x_i \\neq y_i$ and $x_{-i} = y_{-i}$.\n",
    "\n",
    "Note that the probability that a player chooses a certain strategy $x_i$ is non-decreasing (increasing, if $\\eta > 0$) in the payoff associated to that strategy. In particular,\n",
    "\n",
    "- if $\\eta=0$, the players choose strategies independently of the associated payoff (infinite noise, the game basically disappears), i.e., for every two configurations $x, y \\in \\mathcal X$ differing in entry $i$ only, i.e., such that\n",
    "$x_i \\neq y_i$ and $x_{-i} = y_{-i}$,\n",
    "\n",
    "$$\n",
    "\\Lambda_{xy} = \\frac{1}{|\\mathcal{A}|}\n",
    "$$\n",
    "\n",
    "- if $\\eta \\to + \\infty$, then the players randomize among optimal strategies, and assign probability $0$ to suboptimal ones (similar to best response, but not exactly the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous-time noisy best response dynamics in potential games\n",
    "\n",
    "The transition graph of the NBR is strongly connected for each value of $\\eta$, i.e., from every configuration of strategies there is a non-null probability to reach any other configuration of strategies. In particular, for potential games, the NBR admits an invariant distribution\n",
    "\n",
    "$$\n",
    "\\pi_x = \\frac{e^{\\eta \\Phi(x)}}{Z_\\eta}, \\quad Z_\\eta = \\sum_{y\\in \\mathcal{X}} e^{\\eta \\Phi(y)}.\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "This is in contrast with best-response dynamics, which in potential games admits a trapping set of Nash equilibria $\\mathcal{N}_\\infty$ and converge to this set with probability 1 in finite time.\n",
    "\n",
    "**Remark**: note that if $\\eta \\to +\\infty$, the dynamics spends almost all the time in the global maximizers of the potential. However, in general global maximizers of the potential differ in general from $\\mathcal{N}_\\infty$ as defined above, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathcal{N}_\\infty \\neq \\text{argmax}(\\Phi(x)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network games\n",
    "\n",
    "Network games are games where players are associated to nodes of a graph $\\mathcal G$ that describes the interactions among them. In particular, for each player $i\\in\\mathcal V$ and for each couple of configurations $x,y\\in\\mathcal A^{\\mathcal V}$ such that $x_j=y_j$ for each $j\\in\\ N_i\\cup\\{i\\}$ it holds that\n",
    "\n",
    "$$u_i(x)=u_i(y)$$\n",
    "\n",
    "This means that the utility of each player only depends on the actions of its neighbors in the graph $\\mathcal G$.\n",
    "\n",
    "\n",
    "Given an unweighted undirected graph $\\mathcal G=(\\mathcal V,\\mathcal E,W)$ we can construct a network game on it by setting the playersâ€™ utilities to coincide with the weighted sum of the payoffs of the same symmetric two-player game played by the player simultaneously with her neighbors.\n",
    "\n",
    "Specifically, for a symmetric two-player game with action set $\\mathcal A$ and utility function $\\varphi(x_1, x_2)$ we define the network game $(\\mathcal V, \\mathcal A, \\{u_i\\}_{i \\in \\mathcal V} )$ by setting the utility of every player $i \\in \\mathcal V$ as\n",
    "$$\n",
    "u_i(x) = \\sum_{j \\in \\mathcal V} W_{ij}\\varphi(x_i,x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - continuous time NBRD for network coordination game\n",
    "A binary coordination game is a symmetric $2 \\times 2$-game with action set $\\mathcal A = \\{0,1\\}$ with payoff matrix $\\varphi$\n",
    "\n",
    "|   | 0   | 1   |\n",
    "|---|-----|-----|\n",
    "| **0** | a,a | d,c |\n",
    "| **1** | c,d | b,b |\n",
    "\n",
    "where $a > c$ and $b > d$. The inequalities above imply that the best response for each player is to copy the action of the other player.\n",
    "\n",
    "The game is potential, with potential $\\phi$\n",
    "\n",
    "|   | 0   | 1   |\n",
    "|---|-----|-----|\n",
    "| **0** | a-c | 0 |\n",
    "| **1** | 0 | b-d |\n",
    "\n",
    "Consider the **network coordination game**, obtained combining binary coordination games on the link of the following undirected graph\n",
    "\n",
    "![network](network.png)\n",
    "\n",
    "Recall that such network game is potential with potential function $\\Phi$\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\frac{1}{2} \\sum_{i,j} W_{ij} \\phi(x_i,x_j) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Simulate the dynamics with $\\eta=2$ when the coordination game has the same utility value for coordination of action 0 and action 1 respectively, i.e., $a=b=1$ and $c=d=0$. Run the simulation a few times and plot the potential function as a function of time. What do you observe?\n",
    "2. Simulate the dynamics when  instead $a=1$, $b=\\frac{1}{2}$ and $c=d=0$. Run the simulation a few times and plot the potential function as a function of time. What do you observe?\n",
    "3. If the simulation continues for a long time, will the probability for different configurations converge towards some specific values? In that case, how can these be computed?\n",
    "4. Compute the ratio between the probability that after a long time (i.e., in stationarity) all nodes choose action 1 and the probability that all nodes choose action 0, both for case **1** and case **2**. Does this seem to agree with your simulations?\n",
    "5. Compute the ratio between the probability that after a long time (i.e., in stationarity) one node chooses action 1 and the rest choose action 0, and the probability that all nodes choose action 0, both for case **1** and case **2**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "#### Step 1\n",
    "> Simulate the dynamics when the coordination game has the same utility value for coordination of action 0 and action 1 respectively, i.e., $a=b=1$ and $c=d=0$. Run the simulation a few times and plot the potential function as a function of time. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Construct the undirected graph as shown in the picture\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(5))\n",
    "G.add_edges_from([\n",
    "    (0,1),(0,2),\n",
    "    (1,2),(1,3),\n",
    "    (2,3),(2,4),\n",
    "    (3,4)\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(5,7))\n",
    "pos = {3: (0, 0), 4: (1, 0), 1: (0, 1), 2: (1, 1), 0: (0.5, 1.5)}\n",
    "nx.draw(G, pos=pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network coordination game\n",
    "\n",
    "# number of nodes in G = number of players in the game\n",
    "n_players = len(G)\n",
    "\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1\n",
    "c = 0\n",
    "d = 0\n",
    "\n",
    "# utility matrix for the 2x2 coordination game\n",
    "phi = np.array([[a,d],\n",
    "                [c,b]])\n",
    "# the potential function of the 2x2 coordination game\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "\n",
    "# inverse noise parameter\n",
    "eta = 2\n",
    "# available actions\n",
    "actions = [0,1]\n",
    "n_actions = len(actions)\n",
    "# adjacency matrix\n",
    "W = nx.convert_matrix.to_numpy_matrix(G)\n",
    "\n",
    "def utility(player, x, phi):\n",
    "    result = 0\n",
    "    # result is the utility of the player\n",
    "    # for each other player, sum the utility of the 2x2 game between player and other player\n",
    "    for other_player in G.neighbors(player):\n",
    "         result += phi[x[player], x[other_player]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "# Define the noisy best response dynamics\n",
    "\n",
    "# Initialize transition rates matrix\n",
    "n_config = n_actions**n_players\n",
    "Lambda = sp.sparse.lil_matrix((n_config,n_config))\n",
    "\n",
    "# Number of actions for each player\n",
    "n_states =tuple(n_actions for _ in range(n_players))\n",
    "\n",
    "# Fill transition rates matrix\n",
    "for x_id in range(n_config):\n",
    "    x = np.unravel_index(x_id,shape=n_states)\n",
    "    x = np.array(x)\n",
    "    for player in range(n_players):\n",
    "        # compute utilities gained by `player` for each of its possible actions\n",
    "        # while the other players are in the current configuration x\n",
    "        utilities = np.zeros(n_actions)\n",
    "        for action in actions:\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            utilities[action] = utility(player,y,phi)\n",
    "        # exp_utilities contains the exponential of utilities of 'player' for each possibile action\n",
    "        exp_utilities = np.exp(eta*utilities)\n",
    "        for action in actions:\n",
    "            # there are no selfloops \n",
    "            if action == x[player]:\n",
    "                continue\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            y_id = np.ravel_multi_index(tuple(y), dims = n_states)\n",
    "            # add a non-zero rate from x to y proportional to payoff of player when playing action (configuration is y)\n",
    "            Lambda[x_id, y_id] += exp_utilities[action] / np.sum(exp_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the continuous time Markov chain with 2nd approach: local clocks\n",
    "w = np.sum(Lambda, axis=1)\n",
    "\n",
    "# reshape w\n",
    "w = np.array(w.T)[0]\n",
    "for x, weight in enumerate(w):\n",
    "    # add selfloop if a configuration is a sink, otherwise D is not well defined\n",
    "    if weight == 0:\n",
    "        Lambda[x,x] = 1\n",
    "        w[x] = 1\n",
    "D = np.diag(w)\n",
    "P = np.linalg.inv(D) @ Lambda\n",
    "\n",
    "# number of iterations\n",
    "n_steps = 100\n",
    "\n",
    "states = np.zeros(n_steps, dtype=int)\n",
    "# initial configuration is random\n",
    "x = np.random.choice(actions, size = n_players)\n",
    "x_id = np.ravel_multi_index(tuple(x), dims = n_states)\n",
    "states[0] = x_id\n",
    "\n",
    "transition_times = np.zeros(n_steps)\n",
    "t_next = -np.log(np.random.rand())/w[states[0]]\n",
    "\n",
    "for i in range(1,n_steps):\n",
    "    states[i] = np.random.choice(n_config, p=P[states[i-1],:])\n",
    "    transition_times[i] = transition_times[i-1] + t_next\n",
    "    t_next = -np.log(np.random.rand())/w[states[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the evolution of the potential along the simulated \n",
    "# path of the chain\n",
    "\n",
    "# store the potential value at each step of the simulation\n",
    "potentials = np.zeros(n_steps)\n",
    "for step in range(n_steps):\n",
    "    x_id = states[step]\n",
    "    x = np.array(np.unravel_index(x_id, shape = n_states))\n",
    "    for i, j in G.edges:\n",
    "        potentials[step] += pot[x[i],x[j]]\n",
    "\n",
    "plt.step(transition_times, potentials, 'bo-', where=\"post\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better understand the process, visualize the \n",
    "# evolution of the game's configuration in time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the evolution of the game's configurations\n",
    "fig = plt.figure(figsize=(9,12))\n",
    "for t in range(0,min(n_steps,25)):\n",
    "    plt.subplot(5,5,t+1)\n",
    "    x_id = states[t]\n",
    "    x = np.array(np.unravel_index(x_id, shape = n_states))\n",
    "    nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "    nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "    plt.title('jump step = {0}'.format(t+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the potential during most of the\n",
    "time is at its maximum value, corresponding to all nodes choosing the\n",
    "same action. By repeating the simulation, it seems like all nodes tend\n",
    "to end up in the same state after long time, independently of the initialization. The asymptotic configuration consists equally often in that all nodes\n",
    "choose action 0 as that all nodes choose action 1.\n",
    "\n",
    "We know from the theory that the noisy best response dynamics operates a selection among the NE and concentrates, in the small noise limit, most of the probability of its invariant distribution in the subset of those Nash equilibria that correspond to the set of potential maximizers \n",
    "\n",
    "$$\\arg \\max_{x \\in \\mathcal X} \\Phi(x)$$ \n",
    "\n",
    "With the current choice for $a,b,c,d$ the maximum value of the potential $\\Phi$ is realized by both the all-one and all-zero configurations.\n",
    "\n",
    "In this example we have a fixed noisy parameter $\\eta = 2$ but we are still able to observe that the dynamics spends most part of the time in global maxima of the potential. This can be evaluated for instance by computing the average of the potential function along the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = np.diff(transition_times, n=1, append = transition_times[-1] + t_next)\n",
    "avg_potential = np.sum(intervals*potentials)/(transition_times[-1] + t_next)\n",
    "print(\"Average potential:\", avg_potential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to change the noise parameter and see how the dynamics behaves.\n",
    "\n",
    "If $\\eta=0$, then:\n",
    "- the average potential is about 3.5, because transitions are random, the number of links is 7, and the potential counts the number of (undirected) links between nodes with same action. If the nodes choose randomly their state, in expectation the number of links between nodes with same action (and thus the potential) is 3.5.\n",
    "\n",
    "If $\\eta=5$, then:\n",
    "- the dynamics spends most of the time in consensus configuration, thus the potential tends to 7, all the links connect nodes with same action;\n",
    "- the average time between transitions increases, because it is unlikely that an agent modifies her action from a Nash equilibrium to achieve a lower payoff. Notice that the transition times increase only when the transition descends the potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "> Simulate the dynamics when  instead $a=1$, $b=\\frac{1}{2}$ and $c=d=0$. Run the simulation a few times and plot the potential function as a function of time. What do you observe?\n",
    "\n",
    "The same code as previously can be reused, by only changing the parameter $b$. The simulations and the evolution of the potential values along it behave very similarly, with the difference that, among the states in which all nodes choose the same action, state $(0,\\ldots,0)$ is much more frequent than $(1,\\ldots,1)$. Even if all nodes are initialized to choose action 1, with high probability after a while they will all choose action 0 instead. This happens since with the current choice of $a,b,c,d$ the global maximum of the potential $\\Phi$ is realized by the all-zero configuration.\n",
    "\n",
    "**Remark**: note however that, in contrast with the best response dynamics, the noisy best response dynamics still will escape from global maximizers of the potential ('all zeros' states in this example), because the transition graph is irreducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network coordination game\n",
    "\n",
    "# number of nodes in G = number of players in the game\n",
    "n_players = len(G)\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1/2\n",
    "c = 0\n",
    "d = 0\n",
    "# utility matrix for the 2x2 coordination game\n",
    "phi = np.array([[a,d],\n",
    "                [c,b]])\n",
    "# the potential function of the 2x2 coordination game\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "# inverse noise parameter\n",
    "eta = 5\n",
    "# available actions\n",
    "actions = [0,1]\n",
    "n_actions = len(actions)\n",
    "# adjacency matrix\n",
    "W = nx.convert_matrix.to_numpy_matrix(G)\n",
    "\n",
    "def utility(player, x, phi):\n",
    "    result = 0\n",
    "    for other_player in G.neighbors(player):\n",
    "         result += phi[x[player], x[other_player]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the noisy best response dynamics\n",
    "\n",
    "# Initialize transition rates matrix\n",
    "n_config = n_actions**n_players\n",
    "Lambda = sp.sparse.lil_matrix((n_config,n_config))\n",
    "\n",
    "# Number of actions for each player\n",
    "n_states =tuple(n_actions for _ in range(n_players))\n",
    "\n",
    "# Fill transition rates matrix\n",
    "for x_id in range(n_config):\n",
    "    x = np.unravel_index(x_id,shape=n_states)\n",
    "    x = np.array(x)\n",
    "    for player in range(n_players):\n",
    "        # compute utilities gained by `player` for each of its possible actions\n",
    "        # while the other players are in the current configuration x\n",
    "        utilities = np.zeros(n_actions)\n",
    "        for action in actions:\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            utilities[action] = utility(player,y,phi)\n",
    "        exp_utilities = np.exp(eta*utilities)\n",
    "        for action in actions:\n",
    "            if action == x[player]:\n",
    "                continue\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            y_id = np.ravel_multi_index(tuple(y), dims = n_states)\n",
    "            Lambda[x_id, y_id] += exp_utilities[action] / np.sum(exp_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the continuous time Markov chain with 2nd approach: local clocks\n",
    "\n",
    "w = np.sum(Lambda, axis=1)\n",
    "w = np.array(w.T)[0]\n",
    "for x, weight in enumerate(w):\n",
    "    if weight == 0:\n",
    "        Lambda[x,x] = 1\n",
    "        w[x] = 1\n",
    "D = np.diag(w)\n",
    "P = np.linalg.inv(D) @ Lambda\n",
    "\n",
    "# number of iterations\n",
    "n_steps = 100\n",
    "\n",
    "states = np.zeros(n_steps, dtype=int)\n",
    "# initial configuration\n",
    "x = np.random.choice(actions, size = n_players)\n",
    "x_id = np.ravel_multi_index(tuple(x), dims = n_states)\n",
    "states[0] = x_id\n",
    "\n",
    "transition_times = np.zeros(n_steps)\n",
    "t_next = -np.log(np.random.rand())/w[0]\n",
    "\n",
    "for i in range(1,n_steps):\n",
    "    states[i] = np.random.choice(n_config, p=P[states[i-1],:])\n",
    "    transition_times[i] = transition_times[i-1] + t_next\n",
    "    t_next = -np.log(np.random.rand())/w[states[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the evolution of the potential along the simulated \n",
    "# path of the chain\n",
    "\n",
    "# store the potential value at each step of the simulation\n",
    "potentials = np.zeros(n_steps)\n",
    "for step in range(n_steps):\n",
    "    x_id = states[step]\n",
    "    x = np.array(np.unravel_index(x_id, shape = n_states))\n",
    "    for i, j in G.edges:\n",
    "        potentials[step] += pot[x[i],x[j]]\n",
    "\n",
    "plt.step(transition_times, potentials, 'bo-', where=\"post\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better understand the process, visualize the \n",
    "# evolution of the game's configuration in time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the evolution of the game's configurations\n",
    "fig = plt.figure(figsize=(9,12))\n",
    "for t in range(0,min(n_steps,25)):\n",
    "    plt.subplot(5,5,t+1)\n",
    "    x_id = states[t]\n",
    "    x = np.array(np.unravel_index(x_id, shape = n_states))\n",
    "    nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "    nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "    plt.title('jump step = {0}'.format(t+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few observations**\n",
    "\n",
    "- Even if the noisy best response dynamics falls in the global maximum configuration (all zeros), still it will escape from this configuration, for any value of $\\eta$, because the dynamics is ergodic. This is a crucial observation, which distinguishes the noisy best response from the best response.\n",
    "\n",
    "\n",
    "- Also the 'all ones' configuration is a Nash equilibrium, and thus a local maximum of the potential. If $\\eta$ is large and the system falls in the local maximum, it takes a lot to escape this configuration. This is due to the fact that the dynamics becomes \"less ergodic\" as $\\eta$ grows large, in the sense that it takes a lot to escape from Nash equilibria, and in general it takes a lot to descend the potential, because players will accept with small probability transitions that make their payoff decrease, as $\\eta$ grows.\n",
    "\n",
    "\n",
    "- In mathematical terms, this statement may be rephrased as follows: \"as $\\eta$ grows large, the invariant distribution of the dynamics tends to concentrate most of the mass on the maximizers of the potential, but it takes more time to relax to the invariant distribution.\" This is also related to the fact that the transition graphs becomes less connected as $\\eta$ grows (because the probability of an agent choosing a suboptimal strategy tends to $0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to simulate the dynamics from the Nash equilibrium 'all ones' with a large value of $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network coordination game\n",
    "\n",
    "# number of nodes in G = number of players in the game\n",
    "n_players = len(G)\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1/2\n",
    "c = 0\n",
    "d = 0\n",
    "# utility matrix for the 2x2 coordination game\n",
    "phi = np.array([[a,d],\n",
    "                [c,b]])\n",
    "# the potential function of the 2x2 coordination game\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "# inverse noise parameter\n",
    "eta = 10\n",
    "# available actions\n",
    "actions = [0,1]\n",
    "n_actions = len(actions)\n",
    "# adjacency matrix\n",
    "W = nx.convert_matrix.to_numpy_matrix(G)\n",
    "\n",
    "def utility(player, x, phi):\n",
    "    result = 0\n",
    "    for other_player in G.neighbors(player):\n",
    "         result += phi[x[player], x[other_player]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the noisy best response dynamics\n",
    "\n",
    "# Initialize transition rates matrix\n",
    "n_config = n_actions**n_players\n",
    "Lambda = sp.sparse.lil_matrix((n_config,n_config))\n",
    "\n",
    "# Number of actions for each player\n",
    "n_states =tuple(n_actions for _ in range(n_players))\n",
    "\n",
    "# Fill transition rates matrix\n",
    "for x_id in range(n_config):\n",
    "    x = np.unravel_index(x_id,shape=n_states)\n",
    "    x = np.array(x)\n",
    "    for player in range(n_players):\n",
    "        # compute utilities gained by `player` for each of its possible actions\n",
    "        # while the other players are in the current configuration x\n",
    "        utilities = np.zeros(n_actions)\n",
    "        for action in actions:\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            utilities[action] = utility(player,y,phi)\n",
    "        exp_utilities = np.exp(eta*utilities)\n",
    "        for action in actions:\n",
    "            if action == x[player]:\n",
    "                continue\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            y_id = np.ravel_multi_index(tuple(y), dims = n_states)\n",
    "            Lambda[x_id, y_id] += exp_utilities[action] / np.sum(exp_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the continuous time Markov chain with 2nd approach: local clocks\n",
    "\n",
    "w = np.sum(Lambda, axis=1)\n",
    "w = np.array(w.T)[0]\n",
    "for x, weight in enumerate(w):\n",
    "    if weight == 0:\n",
    "        Lambda[x,x] = 1\n",
    "        w[x] = 1\n",
    "D = np.diag(w)\n",
    "P = np.linalg.inv(D) @ Lambda\n",
    "\n",
    "# number of iterations\n",
    "n_steps = 25\n",
    "\n",
    "# initial configuration all ones\n",
    "x_id = n_config-1\n",
    "states[0] = x_id\n",
    "\n",
    "transition_times = np.zeros(n_steps)\n",
    "t_next = -np.log(np.random.rand())/w[0]\n",
    "\n",
    "P_cum = np.cumsum(P, axis=1)\n",
    "\n",
    "for i in range(1,n_steps):\n",
    "    states[i] = np.random.choice(n_config, p=P[states[i-1],:])\n",
    "    transition_times[i] = transition_times[i-1] + t_next\n",
    "    t_next = -np.log(np.random.rand())/w[states[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the evolution of the potential along the simulated \n",
    "# path of the chain\n",
    "\n",
    "# store the potential value at each step of the simulation\n",
    "potentials = np.zeros(n_steps)\n",
    "for step in range(n_steps):\n",
    "    x_id = states[step]\n",
    "    x = np.array(np.unravel_index(x_id, shape = n_states))\n",
    "    for i, j in G.edges:\n",
    "        potentials[step] += pot[x[i],x[j]]\n",
    "\n",
    "plt.step(transition_times, potentials, 'bo-', where=\"post\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, since we used larger $\\eta = 10$, the interval between transitions becomes very large when the system is at maximum potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better understand the process, visualize the \n",
    "# evolution of the game's configuration in time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the evolution of the game's configurations\n",
    "fig = plt.figure(figsize=(9,12))\n",
    "for t in range(0,min(n_steps,25)):\n",
    "    plt.subplot(5,5,t+1)\n",
    "    x_id = states[t]\n",
    "    x = np.array(np.unravel_index(x_id, shape = n_states))\n",
    "    nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "    nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "    plt.title('jump step = {0}'.format(t+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that for a very long time interval the dynamics does not escape from the 'all ones' Nash equilibrium. However, as the simulation is observed for even longer times, the time spent in the 'all ones' configuration is expected to be a negligible fraction of the total time (as shown in the next step). This shows that for large $\\eta$ it takes a lot to escape from local maxima and to relax to the invariant distribution $\\pi$. If one considers instead the BR dynamics, the 'all ones' configuration belongs to $\\mathcal{N}_\\infty$ and the dynamics does not escape from that configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "> If the simulation continues for a long time, will the probability to be in the different configurations converge towards some specific values? In that case, how can these be computed?\n",
    "\n",
    "According to the theory about noisy best response dynamics, this type of dynamics is, for any choice of $\\eta > 0$, an irreducible reversible Markov chain with invariant probability distribution\n",
    "$$\n",
    "\\pi_x = \\frac{e^{\\eta \\Phi(x)}}{Z_\\eta}, \\quad Z_\\eta = \\sum_{y\\in \\mathcal{X}} e^{\\eta \\Phi(y)}.\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Compute the values of $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "> Compute the ratio between the probability that after a long time (i.e., in stationarity) all nodes choose action 1 and the probability that all nodes choose action 0, both for case **1** and case **2**. Does this seem to agree with your simulations?\n",
    "\n",
    "According to (1), we have that \n",
    "$$\n",
    "\\frac{\\pi_{(1,1,1,1,1)}}{\\pi_{(0,0,0,0,0)}} = \\frac{e^{\\eta \\Phi(1,1,1,1,1)}}{e^{\\eta \\Phi(0,0,0,0,0)}}.\n",
    "$$\n",
    "We can thus compute the result according to the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Case 1:\")\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1\n",
    "c = 0\n",
    "d = 0\n",
    "# the potential function of the 2x2 game\n",
    "eta = 2\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "x_0 = [0 for node in G]\n",
    "potential_0 = 0\n",
    "for i, j in G.edges:\n",
    "    potential_0 += 0.5*pot[x_0[i],x_0[j]]\n",
    "\n",
    "x_1 = [1 for node in G]\n",
    "potential_1 = 0\n",
    "for i, j in G.edges:\n",
    "    potential_1 += 0.5*pot[x_1[i],x_1[j]]\n",
    "\n",
    "probability_ratio_1_over_0 = np.exp(eta*potential_1) / np.exp(eta*potential_0)\n",
    "print(\"The ratio between the probability of configuration 'all ones' and configuration 'all zeros' is\", probability_ratio_1_over_0, \"\\n\")\n",
    "\n",
    "print(\"Case 2:\")\n",
    "\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1/2\n",
    "c = 0\n",
    "d = 0\n",
    "# the potential function of the 2x2 game\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "\n",
    "x_0 = [0 for node in G]\n",
    "potential_0 = 0\n",
    "for i, j in G.edges:\n",
    "    potential_0 += pot[x_0[i],x_0[j]]\n",
    "\n",
    "x_1 = [1 for node in G]\n",
    "potential_1 = 0\n",
    "for i, j in G.edges:\n",
    "    potential_1 += pot[x_1[i],x_1[j]]\n",
    "\n",
    "probability_ratio_1_over_0 = np.exp(eta*potential_1) / np.exp(eta*potential_0)\n",
    "print(\"The ratio between the probability of configuration 'all ones' and configuration 'all zeros' is\", probability_ratio_1_over_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if this result agrees with your simulations by estimating $\\pi$ for case **1** and **2** by measuring the time spent in each configuration. Let's do a long simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes in G = number of players in the game\n",
    "n_players = len(G)\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1/2\n",
    "c = 0\n",
    "d = 0\n",
    "# utility matrix for the 2x2 coordination game\n",
    "phi = np.array([[a,d],\n",
    "                [c,b]])\n",
    "# the potential function of the 2x2 coordination game\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "# inverse noise parameter\n",
    "eta = 2\n",
    "# available actions\n",
    "actions = [0,1]\n",
    "n_actions = len(actions)\n",
    "# adjacency matrix\n",
    "W = nx.convert_matrix.to_numpy_matrix(G)\n",
    "\n",
    "def utility(player, x, phi):\n",
    "    result = 0\n",
    "    for other_player in G.neighbors(player):\n",
    "         result += phi[x[player], x[other_player]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the noisy best response dynamics\n",
    "\n",
    "# Initialize transition rates matrix\n",
    "n_config = n_actions**n_players\n",
    "Lambda = sp.sparse.lil_matrix((n_config,n_config))\n",
    "\n",
    "# Number of actions for each player\n",
    "n_states =tuple(n_actions for _ in range(n_players))\n",
    "\n",
    "# Fill transition rates matrix\n",
    "for x_id in range(n_config):\n",
    "    x = np.unravel_index(x_id,shape=n_states)\n",
    "    x = np.array(x)\n",
    "    for player in range(n_players):\n",
    "        # compute utilities gained by `player` for each of its possible actions\n",
    "        # while the other players are in the current configuration x\n",
    "        utilities = np.zeros(n_actions)\n",
    "        for action in actions:\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            utilities[action] = utility(player,y,phi)\n",
    "        exp_utilities = np.exp(eta*utilities)\n",
    "        for action in actions:\n",
    "            if action == x[player]:\n",
    "                continue\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            y_id = np.ravel_multi_index(tuple(y), dims = n_states)\n",
    "            Lambda[x_id, y_id] += exp_utilities[action] / np.sum(exp_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate the continuous time Markov chain with 2nd approach: local clocks\n",
    "\n",
    "w = np.sum(Lambda, axis=1)\n",
    "w = np.array(w.T)[0]\n",
    "for x, weight in enumerate(w):\n",
    "    if weight == 0:\n",
    "        Lambda[x,x] = 1\n",
    "        w[x] = 1\n",
    "D = np.diag(w)\n",
    "P = np.linalg.inv(D) @ Lambda\n",
    "\n",
    "# number of iterations\n",
    "n_steps = 100000\n",
    "\n",
    "states = np.zeros(n_steps, dtype=int)\n",
    "# initial configuration\n",
    "x = np.random.choice(actions, size = n_players)\n",
    "x_id = np.ravel_multi_index(tuple(x), dims = n_states)\n",
    "states[0] = x_id\n",
    "\n",
    "transition_times = np.zeros(n_steps)\n",
    "t_next = -np.log(np.random.rand())/w[0]\n",
    "\n",
    "for i in range(1,n_steps):\n",
    "    states[i] = np.random.choice(n_config, p=P[states[i-1],:])\n",
    "    transition_times[i] = transition_times[i-1] + t_next\n",
    "    t_next = -np.log(np.random.rand())/w[states[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute frequencies\n",
    "intervals = np.diff(transition_times, n=1, append = transition_times[-1] + t_next)\n",
    "frequencies = np.zeros(n_config)\n",
    "\n",
    "for node in range(n_config):\n",
    "    visits = np.argwhere(states == node)\n",
    "    frequencies[node] = np.sum(intervals[visits])/(transition_times[-1] + t_next)\n",
    "print(\"Empirical frequencies:\", frequencies, \"\\n\")\n",
    "\n",
    "print(\"Fraction of time spent in configuration\", np.unravel_index(n_config-1, shape = n_states), \"/ Fraction of time spent in configuration\", np.unravel_index(0, shape = n_states), \":\", frequencies[-1]/frequencies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. \n",
    "> Compute the ratio between the probability that after a long time (i.e., in stationarity) one node chooses action 1 and the rest choose action 0, and the probability that all nodes choose action 0, both for case **1** and case **2**.\n",
    "\n",
    "Using the invariant probability distribution formula (1) again, we can compute the ratio\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i \\in \\mathcal V} \\pi_{\\delta(i)}}{\\pi_{(0,0,0,0,0)}}\n",
    "$$\n",
    "\n",
    "where $\\delta^{(i)}_i=1$ and $\\delta^{(i)}_j=0$, $\\forall j \\neq i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Case 1:\")\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1\n",
    "c = 0\n",
    "d = 0\n",
    "# the potential function of the 2x2 game\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "x_0 = [0 for node in G]\n",
    "potential_0 = 0\n",
    "for i, j in G.edges:\n",
    "    potential_0 += 0.5*pot[x_0[i],x_0[j]]\n",
    "\n",
    "exp_potential_1 = 0\n",
    "for player in G:\n",
    "    x_1 = [0 for node in G]\n",
    "    x_1[player] = 1\n",
    "    # potential_1 is the potential in the configuration where only the player 'player' plays 1 and others play 0\n",
    "    potential_1 = 0\n",
    "    for i, j in G.edges:\n",
    "        potential_1 += 0.5*pot[x_1[i],x_1[j]]\n",
    "    # exp_potential_1 is the sum of the exp of configurations where only one player plays 1\n",
    "    exp_potential_1 += np.exp(eta*potential_1)\n",
    "\n",
    "# print(exp_potential_1)\n",
    "probability_ratio_1_over_0 = exp_potential_1 / np.exp(eta*potential_0)\n",
    "print(\"The ratio between probability of configuration 'one player playing 1, rest playing 0' and configuration (0,0,0,0,0) is\", probability_ratio_1_over_0, \"\\n\")\n",
    "\n",
    "print(\"Case 2:\")\n",
    "\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1/2\n",
    "c = 0\n",
    "d = 0\n",
    "# the potential function of the 2x2 game\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "\n",
    "x_0 = [0 for node in G]\n",
    "potential_0 = 0\n",
    "for i, j in G.edges:\n",
    "    potential_0 += 0.5*pot[x_0[i],x_0[j]]\n",
    "\n",
    "exp_potential_1 = 0\n",
    "for player in G:\n",
    "    x_1 = [0 for node in G]\n",
    "    x_1[player] = 1\n",
    "    potential_1 = 0\n",
    "    for i, j in G.edges:\n",
    "        potential_1 += 0.5*pot[x_1[i],x_1[j]]\n",
    "    exp_potential_1 += np.exp(eta*potential_1)\n",
    "\n",
    "# print(exp_potential_1)\n",
    "probability_ratio_1_over_0 = exp_potential_1 / np.exp(eta*potential_0)\n",
    "print(\"The ratio between probability of configuration 'one player playing 1, rest playing 0' and configuration (0,0,0,0,0) is\", probability_ratio_1_over_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if in case 1 and 2 are different, the ratio \n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i \\in \\mathcal V} \\pi_{\\delta^{(i)}}}{\\pi_{(0,0,0,0,0)}}\n",
    "$$\n",
    "\n",
    "is the same in the two cases.\n",
    "This happens because the potential value of configurations $\\delta^{(i)}$ and $(0,0,0,0,0)$ does not depend on $b$, as the configurations does not contain any couple of neighboring players both playing action $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On the difference between BR and NBR (in potential games)\n",
    "\n",
    "### Irreducibility\n",
    "\n",
    "- The NBR dynamics is irreducible, i.e., from every configuration it can visit every other configuration, even in the small noise limit $\\eta \\to +\\infty$.\n",
    "- The BR is not irreducible. Transitions are allowed only if the potential does not decrease along the trajectory.\n",
    "\n",
    "### Invariant distribution\n",
    "- As a consequence of its irreducibility, the NBR dynamics admits a unique invariant distribution whose is support is the whole space of configurations $\\mathcal{X}$. The invariant distribution is\n",
    "\n",
    "$$\n",
    "\\pi_x = \\frac{e^{\\eta \\Phi(x)}}{Z_\\eta}, \\quad Z_\\eta = \\sum_{y\\in \\mathcal{X}} e^{\\eta \\Phi(y)}.\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "- The BR converges almost surely in finite time to a sink component of Nash equilibria (called recurrent equilibria, indicated by $\\mathcal{N}_\\infty$). However, $\\mathcal{N}_\\infty$ may in general contain multiple disconnected sets of Nash equilibria, and which one is selected depends on the initial condition and on the realization of the Markov process. Thus, the BR dynamics may admit multiple invariant distributions, all of them having support on $\\mathcal{N}_\\infty$.\n",
    "\n",
    "### Nash equilibria selection\n",
    "- As $\\eta \\to +\\infty$, the invariant distribution of the NBR dynamics tends to a uniform probability distribution over $\\text{argmax} \\ \\Phi$, i.e., for large times the NBR dynamics spends a fraction 1 of the total time in global maximizers of the potential. \n",
    "\n",
    "- For large times, the BR dynamics spends a fraction 1 of the total time in $\\mathcal{N}_\\infty$ (in which recurrent equilibria depends on the initial condition and on the realization of the dynamics). \n",
    "\n",
    "- In general $\\mathcal{N}_\\infty \\neq \\text{argmax} \\ \\Phi$. The only property that can be proven is that $\\text{argmax} \\ \\Phi \\subseteq \\mathcal{N}_\\infty$. \n",
    "\n",
    "**Take-home message**: the noisy best reponse with $\\eta \\to +\\infty$ selects a subset of the equilibria selected by the BR dynamics. But the notion of \"selecting an equilibrium\" depends on the considered dynamics, in the sense that the BR dynamics gets trapped in $\\mathcal{N}_\\infty$, while the NBR spends almost of the time in the maximizers of the potential, but can leave the set of the equilibria because of its irreducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, with $a=1, b=1/2$, both $x = \\mathbf{1}$ and $x=\\mathbf{0}$ are consensus states and Nash equilibria of the game. Both of them belong to $\\mathcal{N}_\\infty$. Indeed, if the configuration is $(1,1,1,1,1)$ or $(0,0,0,0,0)$ no rational agents modify their strategy, and the BR dynamics gets stucked in the configuration. However, $\\Phi(1,1,1,1,1)<\\Phi(0,0,0,0,0)$, thus the NBR dynamics with $\\eta \\to \\infty$ for long times (after relaxating to the invariant distribution $\\pi$) will spent almost fraction $1$ of the total time in $(0,0,0,0,0)$. Observe that this does not mean that the configuration $(0,0,0,0,0)$ is trapping for the NBR!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some examples\n",
    "\n",
    "### Majority game (a=b=1, c=d=1) on complete graph: $\\text{argmax} \\ \\Phi = \\mathcal{N}_\\infty = \\mathcal N$.\n",
    "\n",
    "The game admits two Nash equilibria, 'all ones' and 'all zeros', both of them belonging to $\\mathcal{N}_\\infty$ and both of them global maximizers of the potential (in the majority game the potential counts the links between agents playing the same action). \n",
    "\n",
    "Thus: \n",
    "- the BR dynamics will converge almost surely in finite time to one of the two configurations (which one depends crucially on the initial condition);\n",
    "- The NBR with large $\\eta$, observed for large times, will spent fraction 1/2 of time in 'all ones' and 1/2 in 'all zeros', indepedently on the initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.complete_graph(5)\n",
    "\n",
    "pos = nx.circular_layout(G)\n",
    "nx.draw(G, pos=pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the majority game as a network coordination game\n",
    "\n",
    "# number of nodes in G = number of players in the game\n",
    "n_players = len(G)\n",
    "# utility values\n",
    "a = 1\n",
    "b = 1\n",
    "c = 0\n",
    "d = 0\n",
    "# utility matrix for the 2x2 coordination game\n",
    "phi = np.array([[a,d],\n",
    "                [c,b]])\n",
    "# the potential function of the 2x2 coordination game\n",
    "pot = np.array([[a-c,0],\n",
    "                [0,b-d]])\n",
    "# inverse noise parameter\n",
    "eta = 3\n",
    "# available actions\n",
    "actions = [0,1]\n",
    "n_actions = len(actions)\n",
    "# adjacency matrix\n",
    "W = nx.convert_matrix.to_numpy_matrix(G)\n",
    "\n",
    "def utility(player, x, phi):\n",
    "    result = 0\n",
    "    for other_player in G.neighbors(player):\n",
    "         result += phi[x[player], x[other_player]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the noisy best response dynamics\n",
    "\n",
    "# Initialize transition rates matrix\n",
    "n_config = n_actions**n_players\n",
    "Lambda = sp.sparse.lil_matrix((n_config,n_config))\n",
    "\n",
    "# Number of actions for each player\n",
    "n_states =tuple(n_actions for _ in range(n_players))\n",
    "\n",
    "# Fill transition rates matrix\n",
    "for x_id in range(n_config):\n",
    "    x = np.unravel_index(x_id,shape=n_states)\n",
    "    x = np.array(x)\n",
    "    for player in range(n_players):\n",
    "        # compute utilities gained by `player` for each of its possible actions\n",
    "        # while the other players are in the current configuration x\n",
    "        utilities = np.zeros(n_actions)\n",
    "        for action in actions:\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            utilities[action] = utility(player,y,phi)\n",
    "        exp_utilities = np.exp(eta*utilities)\n",
    "        for action in actions:\n",
    "            if action == x[player]:\n",
    "                continue\n",
    "            y = np.array(x)\n",
    "            y[player] = action\n",
    "            y_id = np.ravel_multi_index(tuple(y), dims = n_states)\n",
    "            Lambda[x_id, y_id] += exp_utilities[action] / np.sum(exp_utilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the continuous time Markov chain with 2nd approach: local clocks\n",
    "w = np.sum(Lambda, axis=1)\n",
    "\n",
    "# reshape w\n",
    "w = np.array(w.T)[0]\n",
    "for x, weight in enumerate(w):\n",
    "    # add selfloop if a configuration is a sink, otherwise D is not well defined\n",
    "    if weight == 0:\n",
    "        Lambda[x,x] = 1\n",
    "        w[x] = 1\n",
    "D = np.diag(w)\n",
    "P = np.linalg.inv(D) @ Lambda\n",
    "\n",
    "# number of iterations\n",
    "n_steps = 1000000\n",
    "\n",
    "states = np.zeros(n_steps, dtype=int)\n",
    "# initial configuration\n",
    "x = np.random.choice(actions, size = n_players)\n",
    "x_id = np.ravel_multi_index(tuple(x), dims = n_states)\n",
    "states[0] = x_id\n",
    "\n",
    "transition_times = np.zeros(n_steps)\n",
    "t_next = -np.log(np.random.rand())/w[states[0]]\n",
    "\n",
    "P_cum = np.cumsum(P, axis=1)\n",
    "\n",
    "for i in range(1,n_steps):\n",
    "    states[i] = np.argwhere(P_cum[states[i-1]] > np.random.rand())[0]\n",
    "    transition_times[i] = transition_times[i-1] + t_next\n",
    "    t_next = -np.log(np.random.rand())/w[states[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute frequencies\n",
    "intervals = np.diff(transition_times, n=1, append = transition_times[-1] + t_next)\n",
    "frequencies = np.zeros(n_config)\n",
    "\n",
    "for node in range(n_config):\n",
    "    visits = np.argwhere(states == node)\n",
    "    frequencies[node] = np.sum(intervals[visits])/(transition_times[-1] + t_next)\n",
    "print(\"Empirical frequencies:\", frequencies, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the NBR dynamics spends almost 1/2 of the time on the first configuration, 1/2 on the last configuration (the consensus configurations), and 0 in the other configurations.\n",
    "\n",
    "**Remark**: if the value of $\\eta$ is increased, also the number of steps of the simulation must increase to observe the relaxation to the invariant distribution.\n",
    "\n",
    "The BR dynamics is much easier. Let us plot two examples of trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "G = nx.complete_graph(5)\n",
    "n_nodes = G.number_of_nodes()\n",
    "pos = nx.spring_layout(G) \n",
    "\n",
    "# plot initial condition\n",
    "x = np.array([1,0,1,0,1])\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "x = np.array([1,1,1,0,1])\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "x = np.ones(n_nodes)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "# plot another initial condition\n",
    "x = np.array([1,0,0,0,1])\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "x = np.array([0,0,0,0,1])\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "x = np.zeros(n_nodes)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics always converges to a consensus configuration, and will remain there forever. The empirical frequencies thus converge to $\\pi = \\delta^{(i)}$, where $i$ is either $(0,0,0,0,0)$ or $(1,1,1,1,1)$. Which consensus is selected depends both on the initial condition and the particular realization of the dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority game on barbell graph ($n \\ge 3$): $\\text{argmax} \\ \\Phi \\subset \\mathcal{N}_\\infty = \\mathcal{N}$\n",
    "\n",
    "The game admits four disconnected Nash equilibria: 'all ones', 'all zeros', and 'all zeros/ones in first complete subgraph and all ones/zeros in the second complete graph'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "\n",
    "G = nx.barbell_graph(4,0)\n",
    "n_nodes = G.number_of_nodes()\n",
    "pos = nx.spring_layout(G) \n",
    "\n",
    "# plot first Nash equilibrium\n",
    "x = np.zeros(n_nodes)\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot second Nash equilibrium\n",
    "x = np.ones(n_nodes)\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot third Nash equilibrium\n",
    "x = np.ones(n_nodes)\n",
    "x[0:int(n_nodes/2)] = np.zeros(int(n_nodes/2))\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot fourth Nash equilibrium\n",
    "x = np.ones(n_nodes)\n",
    "x[int(n_nodes/2):n_nodes] = np.zeros(int(n_nodes/2))\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The former two equilibria are global maximizers of the potential, the latter two are not. \n",
    "- All of them belong to $\\mathcal{N}_\\infty$ and no links in the transition graph exist between these four equilibria configurations under the best response dynamics. \n",
    "- Thus, the BR dynamics will converge to one of the equilibria and will be stuck from there on. Instead, the NBR (with large $\\eta$ and large observation time) will spent 1/2 of the time in each of the consensus configurations, and a vanishing time in the other two equilibria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minority game on complete graph (odd $n$): $\\text{argmax} \\ \\Phi = \\mathcal{N}_\\infty = \\mathcal N$\n",
    "\n",
    "In this example all these sets are connected.\n",
    "\n",
    "The game admits many Nash equilibria, that can be distinguished in two classes: the class in which $(n-1)/2$ players play the first strategy, and $(n+1)/2$ players play the second strategy, and the symmetric configurations. \n",
    "\n",
    "All these equilibria are global maximizers of the potential, all of them belong to $\\mathcal{N}_\\infty$ and represent a connected component.\n",
    "\n",
    "Thus, the BR dynamics will converge to this set of equilibria and continue to move between these configurations. The NBR dynamics will converge in distribution to a uniform distribution of probability on these equilibria, but eventually will hit also the other configurations.\n",
    "\n",
    "An example of trajectory of the BR dynamics is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "\n",
    "G = nx.complete_graph(5)\n",
    "n_nodes = G.number_of_nodes()\n",
    "pos = nx.circular_layout(G) \n",
    "\n",
    "# plot first Nash equilibrium\n",
    "x = np.array([0,1,1,0,0])\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot second Nash equilibrium\n",
    "x = np.array([0,1,1,0,1])\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot third Nash equilibrium\n",
    "x = np.array([0,0,1,0,1])\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot fourth Nash equilibrium\n",
    "x = np.array([1,0,1,0,1])\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minority game on a line ($n=4$): $\\text{argmax} \\ \\Phi = \\mathcal{N}_\\infty \\subset \\mathcal{N}$.\n",
    "\n",
    "Each Nash equilibrium satisfies the condition that $x_1 \\neq x_2$, $x_4 \\neq x_3$, otherwise players $1$ and $4$ are not playing best-response. All the other configurations are Nash equilibria.\n",
    "\n",
    "The game admits $4$ Nash equilibria:\n",
    "\n",
    "- (0,1,0,1);\n",
    "- (1,0,1,0);\n",
    "- (0,1,1,0);\n",
    "- (1,0,0,1);\n",
    "\n",
    "Let us plot these configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(0,1),(1,2),(2,3)])\n",
    "n_nodes = G.number_of_nodes()\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# plot first Nash equilibrium\n",
    "x = np.array([0,1,0,1])\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot second Nash equilibrium\n",
    "x = np.array([1,0,1,0])\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot third Nash equilibrium\n",
    "x = np.array([0,1,1,0])\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')\n",
    "\n",
    "# plot fourth Nash equilibrium\n",
    "x = np.array([1,0,0,1])\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==0).T[0].tolist(),\n",
    "        node_color = 'r')\n",
    "nx.draw(G,\n",
    "        pos = pos,\n",
    "        with_labels=True,\n",
    "        nodelist=np.argwhere(x==1).T[0].tolist(),\n",
    "        node_color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first two configurations belong to $\\mathcal{N}_\\infty$. Indeed, both the equilibria are strict (i.e., each player has a unique best-response to this configuration) and thus are sinks for the best response dynamics. \n",
    "- They also maximize the potential, which for the minority game is the number of links between agents playing a different strategy.\n",
    "\n",
    "- The latter two equilibria do not belong to $\\mathcal{N}_\\infty$, i.e., the best response dynamics can escape from them. Indeed, the following transition are allowed: (0,1,1,0) -> (0,0,1,0) -> (1,0,1,0). The dynamics then gets trapped in (1,0,1,0). Thus, the BR dynamics will get stuck in either (1,0,1,0) or (0,1,0,1) in finite time with probability 1. This shows that in general $\\mathcal{N} \\neq \\mathcal{N}_\\infty$.\n",
    "\n",
    "- Since the first two configurations coincide with the maximizers of the potential, the NBR (with large $\\eta$, observed for long times) will spent 1/2 of the time in (0,1,0,1) and 1/2 of the time in (1,0,1,0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
